---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

## Work breakdown

-   Kosiv Marta
-   Ostap Kostiuk
-   Volodymyr Lehchylo-Yanishevskiy

## Introduction

During the first three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the **Bayes
theorem**.

**Naive Bayes Classifier** is a simple algorithm, which is based on
**Bayes theorem** and used for solving classification problems.
**Classification problem** is a problem in which an observation has to
be classified in one of the $n$ classes based on its similarity with
observations in each class.

It is a **probabilistic classifier**, which means it predicts based on
the probability of an observation belonging to each class. To compute
it, this algorithm uses **Bayes' formula,** that you probably already
came across in **Lesson 3:**
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong **independence** assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given observation
(*For example, if an observation is presented as a sentence, then each
word can be a feature*). Thus,
$\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now can be calculated
as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated as corresponding
relative frequencies using available data

## Data description

Our dataset is **Fake news.**

This data set contains data of American news: a headline and an abstract
of the article. Each piece of news is classified as **fake** or
**credible**. The task is to classify the news from test.csv as credible
or fake.

This data set consists of two files: *train.csv* and *test.csv*. The
first one is used to find the probabilities of the corresponding classes
and the second one is used to test your classifier afterwards. Note that
it is crucial to randomly split your data into training and testing
parts to test the classifierʼs possibilities on the unseen data.

```{r}
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
```

## Outline of the work

1.  **Data pre-processing**
2.  **Data visualization**
3.  **Classifier implementation**
4.  **Measurements of effectiveness of your classifier**
5.  **Conclusion**

## Data pre-processing

```{r}
list.files(getwd())
list.files("data/2-fake_news")
```

```{r}
test_path <- "data/2-fake_news/test.csv"
train_path <- "data/2-fake_news/train.csv"

stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split='\n')
splitted_stop_words <- splitted_stop_words[[1]]

```

```{r}
train <-  read.csv(file = train_path, stringsAsFactors = FALSE)
test <-  read.csv(file = test_path, stringsAsFactors = FALSE)

library(tm)
library(Matrix)
library(tidytext)

# Train set
train_bow <- train %>%
  unnest_tokens(word, Body) %>%
  filter(!word %in% splitted_stop_words) %>%
  count(Label, word) %>%
  tidyr::pivot_wider(names_from = word, values_from = n, values_fill = 0)

train_bow_X <- as.matrix(train_bow[,-1])
train_bow_y <- as.character(train_bow$Label)

# Test set
test_bow <- test %>%
  unnest_tokens(word, Body) %>%
  filter(!word %in% splitted_stop_words) %>%
  count(Label, word) %>%
  tidyr::pivot_wider(names_from = word, values_from = n, values_fill = 0)

test_bow_X <- as.matrix(test_bow[,-1])
test_bow_y <- as.character(test_bow$Label)
```

```{r}
# note the power functional features of R bring us
tidy_text <- unnest_tokens(train %>% select(-X, -Headline), 'splitted', 'Body', token="words") %>%
             filter(!splitted %in% splitted_stop_words)

tidy_text %>% count(splitted,sort=TRUE)
```

## Data visualization

```{r}
df <- train

df <- df %>%
  mutate(text = paste(Headline, Body),
         doc_id = dplyr::row_number(),
         Label  = as.factor(Label))

sw <- if (exists("splitted_stop_words")) {
  tolower(splitted_stop_words)
} else {
  tidytext::stop_words$word
}

tidy_text <- df %>%
  dplyr::select(doc_id, Label, text) %>%
  tidytext::unnest_tokens(word, text) %>%
  dplyr::filter(!word %in% sw,
                stringr::str_detect(word, "[a-z]"))

# розподіл класів
df %>%
  dplyr::count(Label) %>%
  ggplot2::ggplot(ggplot2::aes(x = Label, y = n)) +
  ggplot2::geom_col() +
  ggplot2::labs(title = "Розподіл документів за класами (train)",
                x = "Клас", y = "Кількість") +
  ggplot2::theme_minimal()

# топ-20 слів 
tidy_text %>%
  dplyr::count(word, sort = TRUE) %>%
  dplyr::slice_head(n = 20) %>%
  ggplot2::ggplot(ggplot2::aes(x = reorder(word, n), y = n)) +
  ggplot2::geom_col() +
  ggplot2::coord_flip() +
  ggplot2::labs(title = "Топ-20 слів (після чистки)",
                x = NULL, y = "Частота") +
  ggplot2::theme_minimal()

# топ-10 слів у кожному класі
top_by_class <- tidy_text %>%
  dplyr::count(Label, word, sort = TRUE) %>%
  dplyr::group_by(Label) %>%
  dplyr::slice_head(n = 10) %>%
  dplyr::ungroup()

ggplot2::ggplot(top_by_class, ggplot2::aes(x = reorder(word, n), y = n)) +
  ggplot2::geom_col() +
  ggplot2::coord_flip() +
  ggplot2::facet_wrap(~ Label, scales = "free_y") +
  ggplot2::labs(title = "Топ-10 слів у кожному класі",
                x = NULL, y = "Частота") +
  ggplot2::theme_minimal()

# гістограма довжин документів
doc_len <- tidy_text %>%
  dplyr::count(doc_id, name = "len") %>%
  dplyr::left_join(df %>% dplyr::select(doc_id, Label), by = "doc_id")

ggplot2::ggplot(doc_len, ggplot2::aes(x = len, fill = Label)) +
  ggplot2::geom_histogram(bins = 30, alpha = 0.6, position = "identity") +
  ggplot2::labs(title = "Довжини документів (після стоп-слів)",
                x = "Кількість слів", y = "Кількість документів") +
  ggplot2::theme_minimal()
```

## Classifier implementation

```{r}
naiveBayes <- setRefClass(
  "naiveBayes",
  fields = list(
    vocab = "character",
    class_probs = "list",
    word_probs = "list",
    classes = "character"
  ),
  methods = list(
    fit = function(X, y) {
      vocab <<- colnames(X)
      head(vocab)
      class_counts <- table(y)
      class_probs <<- as.list(class_counts / length(y))
      classes <<- names(class_counts)   # зберігаємо тільки реальні класи
      
      word_probs_local <- list()
      V <- length(vocab)
      
      for (cl in names(class_counts)) {
        rows_cl <- which(y == cl)
        counts <- colSums(X[rows_cl, , drop = FALSE])
        total_words <- sum(counts)
        probs <- (counts + 1) / (total_words + V)
        
        word_probs_local[[cl]] <- list(
          probs = probs,
          default = 1 / (total_words + V)
        )
      }
      word_probs <<- word_probs_local
    },
    
    predict = function(messages) {
      messages <- as.character(messages)
      sapply(messages, function(message) {
        # Remove NA or empty messages
        if (is.na(message) || nchar(trimws(message)) == 0) return(NA_character_)
        
        # Tokenize
        tokens <- unnest_tokens(tibble(text = message), word, text, token = "words")$word
        tokens <- tokens[!tokens %in% splitted_stop_words]
        tokens <- tokens[nchar(tokens) > 0]  # ignore empty tokens
        
        log_scores <- sapply(classes, function(cl) {
          log_prior <- log(class_probs[[cl]])
          cond <- word_probs[[cl]]
          log_lik <- if (length(tokens) > 0) {
            sum(sapply(tokens, function(w) {
              if (w %in% names(cond$probs)) log(cond$probs[[w]])
              else log(cond$default)
            }))
          } else {
            0  # no words -> just use prior
          }
          log_prior + log_lik
        })
        
        names(which.max(log_scores))
      })
    },
    
    score = function(X_test, y_test = NULL) {
      if (is.data.frame(X_test)) {
        if (!all(c("text","label") %in% names(X_test))) 
          stop("Data frame must have columns 'text' and 'label'")
        texts <- X_test$text
        labels_true <- as.character(X_test$label)
      } else {
        texts <- as.character(X_test)
        if (is.null(y_test)) stop("Provide y_test when X_test is character vector")
        labels_true <- as.character(y_test)
      }
      
      preds <- predict(texts)
      
      # confusion тільки по справжніх класах
      cm_tbl <- table(
        factor(labels_true, levels = classes),
        factor(preds, levels = classes)
      )
      
      metrics <- tibble(class = classes) %>%
        rowwise() %>%
        mutate(
          TP = cm_tbl[class, class],
          FP = sum(cm_tbl[, class]) - TP,
          FN = sum(cm_tbl[class, ]) - TP,
          TN = sum(cm_tbl) - TP - FP - FN,
          Precision = ifelse((TP + FP) == 0, NA, TP / (TP + FP)),
          Recall = ifelse((TP + FN) == 0, NA, TP / (TP + FN)),
          F1 = ifelse(is.na(Precision) | is.na(Recall) | (Precision + Recall) == 0, NA,
                      2 * Precision * Recall / (Precision + Recall))
        ) %>%
        ungroup()
      
      macro_precision <- mean(metrics$Precision, na.rm = TRUE)
      macro_recall <- mean(metrics$Recall, na.rm = TRUE)
      macro_f1 <- mean(metrics$F1, na.rm = TRUE)
      
      TP_total <- sum(diag(cm_tbl))
      accuracy <- TP_total / sum(cm_tbl)
      
      sum_TP <- sum(metrics$TP)
      sum_FP <- sum(metrics$FP)
      sum_FN <- sum(metrics$FN)
      micro_precision <- ifelse((sum_TP + sum_FP) == 0, NA, sum_TP / (sum_TP + sum_FP))
      micro_recall <- ifelse((sum_TP + sum_FN) == 0, NA, sum_TP / (sum_TP + sum_FN))
      micro_f1 <- ifelse(is.na(micro_precision) | is.na(micro_recall) | (micro_precision + micro_recall) == 0, NA,
                         2 * micro_precision * micro_recall / (micro_precision + micro_recall))
      
      summary_metrics <- list(
        accuracy = accuracy,
        macro = list(precision = macro_precision, recall = macro_recall, f1 = macro_f1),
        micro = list(precision = micro_precision, recall = micro_recall, f1 = micro_f1)
      )
      
      # 1. Confusion Matrix Heatmap
  cm_df <- as.data.frame(cm_tbl)
  names(cm_df) <- c("True", "Predicted", "Count")
  
  p1 <- ggplot(cm_df, aes(x = Predicted, y = True, fill = Count)) +
    geom_tile(color = "white", size = 1.5) +
    geom_text(aes(label = Count), color = "white", size = 6, fontface = "bold") +
    scale_fill_gradient(low = "#3498db", high = "#e74c3c", name = "Count") +
    labs(title = "Confusion Matrix", x = "Predicted Label", y = "True Label") +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
      axis.text = element_text(face = "bold"),
      panel.grid = element_blank()
    )
  
  # 2. Per-Class Metrics Bar Plot
  metrics_long <- metrics %>%
    select(class, Precision, Recall, F1) %>%
    pivot_longer(cols = c(Precision, Recall, F1), names_to = "Metric", values_to = "Value")
  
  p2 <- ggplot(metrics_long, aes(x = class, y = Value, fill = Metric)) +
    geom_bar(stat = "identity", position = "dodge", width = 0.7) +
    geom_text(aes(label = sprintf("%.2f", Value)), 
              position = position_dodge(width = 0.7), 
              vjust = -0.5, size = 3.5) +
    scale_fill_manual(values = c("Precision" = "#2ecc71", "Recall" = "#3498db", "F1" = "#9b59b6")) +
    labs(title = "Per-Class Performance Metrics", x = "Class", y = "Score") +
    ylim(0, 1.1) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
      legend.position = "top",
      legend.title = element_blank()
    )
  
  # 3. Overall Metrics Comparison
  overall_df <- tibble(
    Metric = c("Accuracy", "Macro F1", "Micro F1"),
    Value = c(accuracy, macro_f1, micro_f1),
    Type = c("Overall", "Macro", "Micro")
  )
  
  p3 <- ggplot(overall_df, aes(x = reorder(Metric, Value), y = Value, fill = Type)) +
    geom_bar(stat = "identity", width = 0.6, show.legend = TRUE) +
    geom_text(aes(label = sprintf("%.3f", Value)), hjust = -0.2, size = 5, fontface = "bold") +
    coord_flip() +
    scale_fill_manual(values = c("Overall" = "#e67e22", "Macro" = "#16a085", "Micro" = "#c0392b")) +
    labs(title = "Overall Model Performance", x = "", y = "Score") +
    ylim(0, 1.1) +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
      legend.position = "bottom"
    )
  
  # 4. Prediction Distribution
  pred_dist <- tibble(
    Label = c(rep("True", length(classes)), rep("Predicted", length(classes))),
    Class = rep(classes, 2),
    Count = c(as.numeric(table(factor(labels_true, levels = classes))),
              as.numeric(table(factor(preds, levels = classes))))
  )
  
  p4 <- ggplot(pred_dist, aes(x = Class, y = Count, fill = Label)) +
    geom_bar(stat = "identity", position = "dodge", width = 0.7) +
    geom_text(aes(label = Count), position = position_dodge(width = 0.7), 
              vjust = -0.5, size = 4) +
    scale_fill_manual(values = c("True" = "#34495e", "Predicted" = "#e74c3c")) +
    labs(title = "Class Distribution: True vs Predicted", x = "Class", y = "Count") +
    theme_minimal(base_size = 14) +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 16),
      legend.position = "top",
      legend.title = element_blank()
    )
  
  # Combine all plots
  library(gridExtra)
  combined_plot <- grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2,
                                top = textGrob("Naive Bayes Classification Results", 
                                               gp = gpar(fontsize = 18, fontface = "bold")))
  
  # Print combined plot
  print(combined_plot)
  
  # Return results
  list(
    confusion = cm_tbl, 
    per_class = metrics, 
    summary = summary_metrics, 
    preds = preds,
    plots = list(confusion_matrix = p1, per_class_metrics = p2, 
                 overall_metrics = p3, distribution = p4)
  )
    }
  )
)


```

## Measure effectiveness of your classifier

-   Confusion matrix:

    -   A ***true positive*** result is one where the model correctly
        predicts the positive class.
    -   A ***true negative*** result is one where the model correctly
        predicts the negative class.
    -   A ***false positive*** result is one where the model incorrectly
        predicts the positive class when it is actually negative.
    -   A ***false negative*** result is one where the model incorrectly
        predicts the negative class when it is actually positive.

    Precision measures the proportion of true positive predictions among
    all positive predictions made by the model.

    $$
    Precision = \frac{TP}{TP+FP}
    $$

    Recall, on the other hand, measures the proportion of true positives
    identified out of all actual positive cases.

    $$
    Recall = \frac{TP}{TP+FN}
    $$

    F1 score is the harmonic mean of both precision and recall.

    $$
    F1 = \frac{2\times Precision \times Recall}{Precision + Recall}
    $$

-   Visualize them.

-   Show failure cases.

```{r}
# Initialize classifier
nb <- naiveBayes$new()

# Fit model
nb$fit(train_bow_X, train_bow_y)

# Predict single text
sample_text <- "Breaking news: new study shows dramatic effects of climate change."
pred_class <- nb$predict(sample_text)

cat("\n=== Prediction Demo ===\n")
cat("Sample text:", sample_text, "\n")
cat("Predicted class:", pred_class, "\n\n")

# Score on test set
score_res <- nb$score(test$Body, test$Label)

# Overall metrics
cat("=== Overall Metrics ===\n")
cat("Accuracy:", round(score_res$summary$accuracy, 4), "\n")
cat("Macro - Precision:", round(score_res$summary$macro$precision, 4), 
    "Recall:", round(score_res$summary$macro$recall, 4),
    "F1:", round(score_res$summary$macro$f1, 4), "\n")
cat("Micro - Precision:", round(score_res$summary$micro$precision, 4), 
    "Recall:", round(score_res$summary$micro$recall, 4),
    "F1:", round(score_res$summary$micro$f1, 4), "\n\n")

# Per-class metrics
cat("=== Per-Class Metrics ===\n")
print(score_res$per_class %>% select(class, Precision, Recall, F1))
cat("\n")

# Confusion matrix
cat("=== Confusion Matrix ===\n")
print(score_res$confusion)
cat("\n")



```

## Conclusions

#### Method

In this project, we implemented a Naive Bayes classifier for text
classification tasks. The classifier is based on Bayes' theorem, which
calculates the posterior probability of each class given the observed
features (words in text).

The "naive" assumption is that all words are conditionally independent.

To prevent numerical underflow, we work in log space.

$$
logP(class∣text)=logP(class)+i=1∑n​logP(wordi​∣class)
$$

We also applied Laplace smoothing (add-one smoothing) to handle unseen
words.

#### Pros and cons

**Advantages:**

-   Fast training and prediction - requires only counting word
    frequencies

-   That implies it works well with small datasets

**Disadvantages:**

-   Strong independence assumption rarely holds in real text (words are
    often correlated)

-   Sensitive to irrelevant features

-   Cannot capture word order or context (bag-of-words limitation)

-   Performance degrades if training data is very imbalanced

#### Why F1?

**Accuracy** is misleading when classes are imbalanced. For example, if
95% of news articles are "real" and only 5% are "fake", a classifier
that always predicts "real" achieves 95% accuracy but is completely
useless for detecting fake news.

**F1 score** is preferable because it balances **Precision** and
**Recall**:

F1 = 2 × (Precision × Recall) / (Precision + Recall)
